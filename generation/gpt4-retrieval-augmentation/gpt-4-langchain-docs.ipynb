{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HDKlQO5svqI",
        "outputId": "4a57df82-5e46-4b60-f0c7-c408e3d0f5b0"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU bs4 tiktoken openai langchain pinecone-client[grpc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo9gYhGPr_DQ",
        "outputId": "f1b00acf-b7f0-48e3-abf6-8b0d8a86ffd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "res = requests.get(\"https://langchain.readthedocs.io/en/latest/\")\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n40_0MtlsKgM",
        "outputId": "e0978f3f-2b1a-4d95-c8c6-5e73a5f35f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".rst .pdf Welcome to LangChain Contents Getting Started Modules Use Cases Reference Docs LangChain Ecosystem Additional Resources Welcome to LangChain# Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. But using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you are able to combine them with other sources of computation or knowledge. This library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include: ‚ùì Question Answering over specific documents Documentation End-to-end Example: Question Answering over Notion Database üí¨ Chatbots Documentation End-to-end Example: Chat-LangChain ü§ñ Agents Documentation End-to-end Example: GPT+WolframAlpha Getting Started# Checkout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application. Getting Started Documentation Modules# There are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing order of complexity: Prompts: This includes prompt management, prompt optimization, and prompt serialization. LLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs. Document Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources. Utils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application. Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. Indexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that. Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents. Memory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory. Chat: Chat models are a variation on Language Models that expose a different API - rather than working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above. Use Cases# The above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports. Agents: Agents are systems that use a language model to interact with other tools. These can be used to do more grounded question/answering, interact with APIs, or even take actions. Chatbots: Since language models are good at producing text, that makes them ideal for creating chatbots. Data Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources. Question Answering: Answering questions over specific documents, only utilizing the information in those documents to construct an answer. A type of Data Augmented Generation. Summarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation. Querying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page. Evaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this. Generate similar examples: Generating similar examples to a given input. This is a common use case for many applications, and LangChain provides some prompts/chains for assisting in this. Compare models: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so. Reference Docs# All of LangChain‚Äôs reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain. Reference Documentation LangChain Ecosystem# Guides for how other companies/products can be used with LangChain LangChain Ecosystem Additional Resources# Additional collection of resources we think may be useful as you develop your application! LangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents. Glossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not! Gallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications. Deployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps. Discord: Join us on our Discord to discuss all things LangChain! Tracing: A guide on using tracing in LangChain to visualize the execution of chains and agents. Production Support: As you move your LangChains into production, we‚Äôd love to offer more comprehensive support. Please fill out this form and we‚Äôll set up a dedicated support Slack channel. next Quickstart Guide Contents Getting Started Modules Use Cases Reference Docs LangChain Ecosystem Additional Resources By Harrison Chase ¬© Copyright 2023, Harrison Chase. Last updated on Mar 24, 2023.\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "import html\n",
        "import re\n",
        "\n",
        "domain = \"https://langchain.readthedocs.io/\"\n",
        "domain_full = domain + \"en/latest/\"\n",
        "\n",
        "soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "# Find all links to local pages on the website\n",
        "local_links = []\n",
        "for link in soup.find_all('a', href=True):\n",
        "    href = link['href']\n",
        "    if href.startswith(domain) or href.startswith('./') \\\n",
        "        or href.startswith('/') or href.startswith('modules') \\\n",
        "        or href.startswith('use_cases'):\n",
        "        local_links.append(urllib.parse.urljoin(domain_full, href))\n",
        "\n",
        "# Find the main content using CSS selectors\n",
        "main_content = soup.select('body main')[0]\n",
        "\n",
        "# Extract the HTML code of the main content\n",
        "main_content_html = str(main_content)\n",
        "\n",
        "# Extract the plaintext of the main content\n",
        "main_content_text = main_content.get_text()\n",
        "\n",
        "# Remove all HTML tags\n",
        "main_content_text = re.sub(r'<[^>]+>', '', main_content_text)\n",
        "\n",
        "# Remove extra white space\n",
        "main_content_text = ' '.join(main_content_text.split())\n",
        "\n",
        "# Replace HTML entities with their corresponding characters\n",
        "main_content_text = html.unescape(main_content_text)\n",
        "\n",
        "print(main_content_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OcIkny_6xiZJ"
      },
      "outputs": [],
      "source": [
        "def scrape(url: str):\n",
        "    res = requests.get(url)\n",
        "    if res.status_code != 200:\n",
        "        print(f\"{res.status_code} for '{url}'\")\n",
        "        return None\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "    # Find all links to local pages on the website\n",
        "    local_links = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        if href.startswith(domain) or href.startswith('./') \\\n",
        "            or href.startswith('/') or href.startswith('modules') \\\n",
        "            or href.startswith('use_cases'):\n",
        "            local_links.append(urllib.parse.urljoin(domain_full, href))\n",
        "\n",
        "    # Find the main content using CSS selectors\n",
        "    main_content = soup.select('body main')[0]\n",
        "\n",
        "    # Extract the HTML code of the main content\n",
        "    main_content_html = str(main_content)\n",
        "\n",
        "    # Extract the plaintext of the main content\n",
        "    main_content_text = main_content.get_text()\n",
        "\n",
        "    # Remove all HTML tags\n",
        "    main_content_text = re.sub(r'<[^>]+>', '', main_content_text)\n",
        "\n",
        "    # Remove extra white space\n",
        "    main_content_text = ' '.join(main_content_text.split())\n",
        "\n",
        "    # Replace HTML entities with their corresponding characters\n",
        "    main_content_text = html.unescape(main_content_text)\n",
        "\n",
        "    # return as json\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"text\": main_content_text\n",
        "    }, local_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGUGao9_uNH3",
        "outputId": "1d4a4e02-7ace-49a9-98d5-f5b4304779b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://langchain.readthedocs.io/en/latest/\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/s3_file.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/vectorstore_examples/elasticsearch.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/agents/examples/human_tools.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/url.html\n",
            "https://langchain.readthedocs.io/en/latest/use_cases/evaluation/qa_benchmarking_pg.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/gutenberg.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/prompts/how_to_guides.html\n",
            "https://langchain.readthedocs.io/en/latest/examples/custom_prompt_template.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/custom_prompt_template.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/memory/getting_started.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/memory/types/buffer_window.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/integrations/gooseai_example.html\n",
            "https://langchain.readthedocs.io/en/latest/use_cases/evaluation/data_augmented_question_answering.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/vectorstore_examples/milvus.html\n",
            "https://langchain.readthedocs.io/en/latest/use_cases/model_laboratory.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/agents/examples/intermediate_steps.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/analyze_document.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/memory/types/kg.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/vectorstore_examples/pgvector.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/examples/textsplitter.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/utils/examples/wolfram_alpha.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/integrations/banana.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/agents/examples/max_iterations.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/utils/how_to_guides.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chat/examples/promptlayer_chatopenai.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/integrations/modal.html\n",
            "https://langchain.readthedocs.io/en/latest/examples/python.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/python.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/prompts/getting_started.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/generic/from_hub.html\n",
            "https://langchain.readthedocs.io/en/latest/examples/example_selectors.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/example_selectors.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/integrations/self_hosted_examples.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/retriever_examples/chatgpt-plugin-retriever.html\n",
            "https://langchain.readthedocs.io/en/latest/ecosystem.html\n",
            "https://langchain.readthedocs.io/en/latest/use_cases/agents.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/memory/examples/adding_memory_chain_multiple_inputs.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/utils/examples/bing_search.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/image.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/vector_db_qa_with_sources.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/CoNLL-U.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/integrations/petals_example.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/vectorstore_examples/weaviate.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/examples/llm_summarization_checker.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/examples/pal.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/agents/implementations/self_ask_with_search.html\n",
            "https://langchain.readthedocs.io/en/latest/use_cases/extraction.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/utils/examples/zapier.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/generic/sequential_chains.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/examples/llm_serialization.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/generic_how_to.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/figma.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/generic/transformation.html\n",
            "https://langchain.readthedocs.io/en/latest/use_cases/evaluation.html\n",
            "https://langchain.readthedocs.io/en/latest/evaluation/agent_vectordb_sota_pg.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/evaluation/agent_vectordb_sota_pg.html'\n",
            "https://langchain.readthedocs.io/en/latest/evaluation/huggingface_datasets.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/evaluation/huggingface_datasets.html'\n",
            "https://langchain.readthedocs.io/en/latest/evaluation/data_augmented_question_answering.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/evaluation/data_augmented_question_answering.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/integrations/manifest.html\n",
            "https://langchain.readthedocs.io/en/latest/generic/transformation.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/generic/transformation.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/integrations/ai21.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/agents/examples/custom_tools.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/integrations/forefrontai_example.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/examples/constitutional_chain.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/vectorstore_examples/faiss.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/getting_started.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/gitbook.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/s3_directory.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/examples/hyde.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/key_concepts.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/pdf.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/ifixit.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/utils/examples/google_serper.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/memory/examples/chatgpt_clone.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/memory/types/summary.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/examples/llm_requests.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/examples/llm_math.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/copypaste.html\n",
            "https://langchain.readthedocs.io/en/latest/examples/bing_search.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/bing_search.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/llms/key_concepts.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/memory/types/token_buffer.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/roam.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/indexes/vectorstore_examples/deeplake.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/memory/types/buffer.html\n",
            "https://langchain.readthedocs.io/en/latest/evaluation/qa_generation.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/evaluation/qa_generation.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/agents/examples/chat_conversation_agent.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/agents/agent_toolkits/json.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/memory/how_to_guides.html\n",
            "https://langchain.readthedocs.io/en/latest/types/kg.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/types/kg.html'\n",
            "https://langchain.readthedocs.io/en/latest/types/entity_summary_memory.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/types/entity_summary_memory.html'\n",
            "https://langchain.readthedocs.io/en/latest/examples/conversational_customization.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/conversational_customization.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/examples/moderation.html\n",
            "https://langchain.readthedocs.io/en/latest/evaluation/qa_benchmarking_pg.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/evaluation/qa_benchmarking_pg.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/utility_how_to.html\n",
            "https://langchain.readthedocs.io/en/latest/examples/api.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/api.html'\n",
            "https://langchain.readthedocs.io/en/latest/examples/llm_checker.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/llm_checker.html'\n",
            "https://langchain.readthedocs.io/en/latest/examples/sqlite.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/sqlite.html'\n",
            "https://langchain.readthedocs.io/en/latest/examples/pal.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/pal.html'\n",
            "https://langchain.readthedocs.io/en/latest/modules/chains/examples/api.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/utils/examples/serpapi.html\n",
            "https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/airbyte_json.html\n",
            "https://langchain.readthedocs.io/en/latest/getting_started.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/getting_started.html'\n",
            "https://langchain.readthedocs.io/en/latest/examples/partial.html\n",
            "404 for 'https://langchain.readthedocs.io/en/latest/examples/partial.html'\n"
          ]
        }
      ],
      "source": [
        "links = [\"https://langchain.readthedocs.io/en/latest/\"]\n",
        "scraped = set()\n",
        "data = []\n",
        "\n",
        "while True:\n",
        "    if len(links) == 0:\n",
        "        print(\"Complete\")\n",
        "        break\n",
        "    url = links[0]\n",
        "    print(url)\n",
        "    res = scrape(url)\n",
        "    scraped.add(url)\n",
        "    if res is not None:\n",
        "        page_content, local_links = res\n",
        "        data.append(page_content)\n",
        "        # add new links to links list\n",
        "        links.extend(local_links)\n",
        "        # remove duplicates\n",
        "        links = list(set(links))\n",
        "    # remove links already scraped\n",
        "    links = [link for link in links if link not in scraped]\n",
        "\n",
        "    if len(scraped) >= 100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0tUQRxtzqF0",
        "outputId": "a7a9b799-98cb-41a2-a696-fc9f00579773"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'url': 'https://langchain.readthedocs.io/en/latest/modules/agents/examples/human_tools.html',\n",
              " 'text': '.ipynb .pdf Human as a tool Human as a tool# Human are AGI so they can certainly be used as a tool to help out AI agent when it is confused. import sys from langchain.chat_models import ChatOpenAI from langchain.llms import OpenAI from langchain.agents import load_tools, initialize_agent llm = ChatOpenAI(temperature=0.0) math_llm = OpenAI(temperature=0.0) tools = load_tools( [\"human\", \"llm-math\"], llm=math_llm, ) agent_chain = initialize_agent( tools, llm, agent=\"zero-shot-react-description\", verbose=True, ) In the above code you can see the tool takes input directly from command line. You can customize prompt_func and input_func according to your need. agent_chain.run(\"What is Eric Zhu\\'s birthday?\") # Answer with \"last week\" > Entering new AgentExecutor chain... I don\\'t know Eric Zhu, so I should ask a human for guidance. Action: Human Action Input: \"Do you know when Eric Zhu\\'s birthday is?\" Do you know when Eric Zhu\\'s birthday is? last week Observation: last week Thought:That\\'s not very helpful. I should ask for more information. Action: Human Action Input: \"Do you know the specific date of Eric Zhu\\'s birthday?\" Do you know the specific date of Eric Zhu\\'s birthday? august 1st Observation: august 1st Thought:Now that I have the date, I can check if it\\'s a leap year or not. Action: Calculator Action Input: \"Is 2021 a leap year?\" Observation: Answer: False Thought:I have all the information I need to answer the original question. Final Answer: Eric Zhu\\'s birthday is on August 1st and it is not a leap year in 2021. > Finished chain. \"Eric Zhu\\'s birthday is on August 1st and it is not a leap year in 2021.\" previous Defining Custom Tools next Intermediate Steps By Harrison Chase ¬© Copyright 2023, Harrison Chase. Last updated on Mar 24, 2023.'}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouY4rcx7z2oa"
      },
      "source": [
        "It's pretty ugly but it's good enough for now. Let's see how we can process all of these. We will chunk everything into ~1000 token chunks, we can do this easily with `langchain` and `tiktoken`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Rb7KxUqYzsuV"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('p50k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(text, disallowed_special=())\n",
        "    \n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "OKO8e3Dp0dQS"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=20,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLdvW8eq06Zd"
      },
      "source": [
        "Process the `data` into more chunks using this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fe4aa5160ef74ecd820f9c6f7de035a0",
            "8f0edead487948358efe478a01316209",
            "bcd3e274345a44dc950890c1fa1026a7",
            "153f898146264d50b77b5ef23db92408",
            "30d7236e54844058b7c76404e1f2ccb8",
            "348fe1bd02ed4dca8df422031d1184f6",
            "157f79e1ecf0423393cb15dcd2e66996",
            "a43a7db5cab149b3a8aeef23d3fb936f",
            "865d0f1e70cc4889aaf91cf1ad82b909",
            "1bed5d4ebf054e80b4d63d6f8a2593d8",
            "04fd6e9cebaa4c9287d16cb8c861c8a3"
          ]
        },
        "id": "uOdPyiAQ0uWs",
        "outputId": "b1d00544-a432-4a41-a68e-59a0f9825070"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "beea084859ee401b9e5af6ffc6bc0544",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/81 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "chunks = []\n",
        "\n",
        "for idx, record in enumerate(tqdm(data)):\n",
        "    texts = text_splitter.split_text(record['text'])\n",
        "    chunks.extend([{\n",
        "        'id': str(uuid4()),\n",
        "        'text': texts[i],\n",
        "        'chunk': i,\n",
        "        'url': record['url']\n",
        "    } for i in range(len(texts))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "369"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'id': '353f24c1-f1f0-4aac-b0fa-b6e1f2c5b8cf',\n",
              "  'text': '.rst .pdf Welcome to LangChain Contents Getting Started Modules Use Cases Reference Docs LangChain Ecosystem Additional Resources Welcome to LangChain# Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. But using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you are able to combine them with other sources of computation or knowledge. This library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include: ‚ùì Question Answering over specific documents Documentation End-to-end Example: Question Answering over Notion Database üí¨ Chatbots Documentation End-to-end Example: Chat-LangChain ü§ñ Agents Documentation End-to-end Example: GPT+WolframAlpha Getting Started# Checkout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application. Getting Started Documentation Modules# There are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing order of complexity: Prompts: This includes prompt management, prompt',\n",
              "  'chunk': 0,\n",
              "  'url': 'https://langchain.readthedocs.io/en/latest/'},\n",
              " {'id': 'c593df84-a0f6-4965-99c5-cdb2726ed548',\n",
              "  'text': 'Prompts: This includes prompt management, prompt optimization, and prompt serialization. LLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs. Document Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources. Utils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application. Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. Indexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that. Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents. Memory: Memory is the',\n",
              "  'chunk': 1,\n",
              "  'url': 'https://langchain.readthedocs.io/en/latest/'},\n",
              " {'id': '5d68d5e1-5915-4321-bf98-4b35f5ac4d51',\n",
              "  'text': 'of end to end agents. Memory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory. Chat: Chat models are a variation on Language Models that expose a different API - rather than working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above. Use Cases# The above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports. Agents: Agents are systems that use a language model to interact with other tools. These can be used to do more grounded question/answering, interact with APIs, or even take actions. Chatbots: Since language models are good at producing text, that makes them ideal for creating chatbots. Data Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources. Question Answering: Answering questions over',\n",
              "  'chunk': 2,\n",
              "  'url': 'https://langchain.readthedocs.io/en/latest/'}]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JegURaAg2PuN"
      },
      "source": [
        "Our chunks are ready so now we move onto embedding and indexing everything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGIZbQqJ2WBh"
      },
      "source": [
        "## Initialize Embedding Model\n",
        "\n",
        "We use `text-embedding-ada-002` as the embedding model. We can embed text like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "kteZ69Z5M55S"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key: \")\n",
        "\n",
        "# initialize openai API key\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]  #platform.openai.com\n",
        "\n",
        "embed_model = \"text-embedding-ada-002\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[\n",
        "        \"Sample document text goes here\",\n",
        "        \"there will be several phrases in each batch\"\n",
        "    ],\n",
        "    engine=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNZ7IWekNLbu"
      },
      "source": [
        "In the response `res` we will find a JSON-like object containing our new embeddings within the `'data'` field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esagZj6iNLPZ",
        "outputId": "0e8b59d7-6c26-4fbf-e093-56d35aa18ab5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['object', 'data', 'model', 'usage'])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zStnHFpkNVIU"
      },
      "source": [
        "Inside `'data'` we will find two records, one for each of the two sentences we just embedded. Each vector embedding contains `1536` dimensions (the output dimensionality of the `text-embedding-ada-002` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVoP9VcINWAC",
        "outputId": "36329c98-2191-4e3d-b064-60af9b905dae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(res['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-zraDCjNeC6",
        "outputId": "a7c79486-6fb4-4bc6-efec-320e9f525766"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1536, 1536)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPd41MjANhmp"
      },
      "source": [
        "We will apply this same embedding logic to the langchain docs dataset we've just scraped. But before doing so we must create a place to store the embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPi4MZvMNvUH"
      },
      "source": [
        "## Initializing the Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5RRQArrN2lN"
      },
      "source": [
        "Now we need a place to store these embeddings and enable a efficient vector search through them all. To do that we use Pinecone, we can get a [free API key](https://app.pinecone.io/) and enter it below where we will initialize our connection to Pinecone and create a new index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pinecone\n",
        "\n",
        "# os.environ[\"PINECONE_API_KEY\"] = getpass(\"Pinecone API Key: \")\n",
        "# os.environ[\"PINECONE_ENVIRONMENT\"] = input(\"Pinecone Environment: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO8sbJFZNyIZ",
        "outputId": "f2d2efca-65be-47ea-ab1d-1dab2786a6b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_name = 'gpt-4-langchain-docs'\n",
        "\n",
        "# initialize connection to pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.environ[\"PINECONE_API_KEY\"],  # app.pinecone.io (console)\n",
        "    environment=os.environ[\"PINECONE_ENVIRONMENT\"]  # next to API key in console\n",
        ")\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # if does not exist, create index\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(res['data'][0]['embedding']),\n",
        "        metric='dotproduct'\n",
        "    )\n",
        "# connect to index\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezSTzN2rPa2o"
      },
      "source": [
        "We can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI `text-embedding-ada-002` built embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "369"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "min(len(chunks), 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 100, 200, 300]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(range(0, len(chunks), batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b6b5865b02504e10a020ad5f42241df6",
            "b1489d5d6c1f498fadaea8aeb16ab60f",
            "90aa35cbdf0c45a1bb7b9075c48a6f7d",
            "70eaf7d1a5b24e49a32490fd3a75ea15",
            "144e2e3c8a014c549e0f552a64a670ef",
            "e5b49411f2134a9b9649528314f746d6",
            "7d78613ce91b4427a4afacb699ef031e",
            "1b93eb9d358041ab99fe87045f7f0660",
            "af4f336bfcb446afb9e6a513d49d791f",
            "a9552f4dca1642e2924ee152067f1f3d",
            "c82f8fbcef0648489f1dcbb4af5ea8c4"
          ]
        },
        "id": "iZbFbulAPeop",
        "outputId": "a017780a-19d0-4e6f-a68c-529c0c96e4f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0ca373827f04ce0a6813eca58987cdc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import datetime\n",
        "from time import sleep\n",
        "\n",
        "batch_size = 100  # how many embeddings we create and insert at once\n",
        "\n",
        "for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "\n",
        "    # find end of batch\n",
        "    i_end = min(len(chunks), i+batch_size)\n",
        "    meta_batch = chunks[i:i_end]\n",
        "\n",
        "    # get ids\n",
        "    ids_batch = [x['id'] for x in meta_batch]\n",
        "\n",
        "    # get texts to encode\n",
        "    texts = [x['text'] for x in meta_batch]\n",
        "    \n",
        "    # create embeddings (try-except added to avoid RateLimitError)\n",
        "    try:\n",
        "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "    except:\n",
        "        done = False\n",
        "        while not done:\n",
        "            sleep(5)\n",
        "            try:\n",
        "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
        "                done = True\n",
        "            except:\n",
        "                pass\n",
        "    embeds = [record['embedding'] for record in res['data']]\n",
        "\n",
        "    # cleanup metadata\n",
        "    meta_batch = [{\n",
        "        'text': x['text'],\n",
        "        'chunk': x['chunk'],\n",
        "        'url': x['url']\n",
        "    } for x in meta_batch]\n",
        "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
        "    \n",
        "    # upsert to Pinecone\n",
        "    index.upsert(vectors=to_upsert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YttJOrEtQIF9"
      },
      "source": [
        "Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation using GPT-4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FumVmMRlQQ7w"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLRODeL-QTJ9"
      },
      "source": [
        "To search through our documents we first need to create a query vector `xq`. Using `xq` we will retrieve the most relevant chunks from the LangChain docs, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "FMUPdX9cQQYC"
      },
      "outputs": [],
      "source": [
        "query = \"how do I use the LLMChain in LangChain?\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[query],\n",
        "    engine=embed_model\n",
        ")\n",
        "\n",
        "# retrieve from Pinecone\n",
        "xq = res['data'][0]['embedding']\n",
        "\n",
        "# get relevant contexts (including the questions)\n",
        "res = index.query(xq, top_k=5, include_metadata=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl9SrFPkQjg-",
        "outputId": "86a8c598-15d1-4ad1-db32-6db2b3e0af1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'matches': [{'id': '7c2c12b3-0b54-4300-b357-734246eefbc5',\n",
              "              'metadata': {'chunk': 1.0,\n",
              "                           'text': 'chain first uses a LLM to construct the '\n",
              "                                   'url to hit, then makes that request with '\n",
              "                                   'the Requests wrapper, and finally runs '\n",
              "                                   'that result through the language model '\n",
              "                                   'again in order to product a natural '\n",
              "                                   'language response. Example Notebook '\n",
              "                                   'LLMBash Chain Links Used: BashProcess, '\n",
              "                                   'LLMChain Notes: This chain takes user '\n",
              "                                   'input (a question), uses an LLM chain to '\n",
              "                                   'convert it to a bash command to run in the '\n",
              "                                   'terminal, and then returns that as the '\n",
              "                                   'result. Example Notebook LLMChecker Chain '\n",
              "                                   'Links Used: LLMChain Notes: This chain '\n",
              "                                   'takes user input (a question), uses an LLM '\n",
              "                                   'chain to answer that question, and then '\n",
              "                                   'uses other LLMChains to self-check that '\n",
              "                                   'answer. Example Notebook LLMRequests Chain '\n",
              "                                   'Links Used: Requests, LLMChain Notes: This '\n",
              "                                   'chain takes a URL and other inputs, uses '\n",
              "                                   'Requests to get the data at that URL, and '\n",
              "                                   'then passes that along with the other '\n",
              "                                   'inputs into an LLMChain to generate a '\n",
              "                                   'response. The example included shows how '\n",
              "                                   'to ask a question to Google - it firsts '\n",
              "                                   'constructs a Google url, then fetches the '\n",
              "                                   'data there, then passes that data + the '\n",
              "                                   'original question into an LLMChain to get '\n",
              "                                   'an answer. Example Notebook Moderation '\n",
              "                                   'Chain Links Used: LLMChain, '\n",
              "                                   'ModerationChain Notes: This chain shows '\n",
              "                                   'how to use OpenAI‚Äôs content',\n",
              "                           'url': 'https://langchain.readthedocs.io/en/latest/modules/chains/utility_how_to.html'},\n",
              "              'score': 0.8451743,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': 'c593df84-a0f6-4965-99c5-cdb2726ed548',\n",
              "              'metadata': {'chunk': 1.0,\n",
              "                           'text': 'Prompts: This includes prompt management, '\n",
              "                                   'prompt optimization, and prompt '\n",
              "                                   'serialization. LLMs: This includes a '\n",
              "                                   'generic interface for all LLMs, and common '\n",
              "                                   'utilities for working with LLMs. Document '\n",
              "                                   'Loaders: This includes a standard '\n",
              "                                   'interface for loading documents, as well '\n",
              "                                   'as specific integrations to all types of '\n",
              "                                   'text data sources. Utils: Language models '\n",
              "                                   'are often more powerful when interacting '\n",
              "                                   'with other sources of knowledge or '\n",
              "                                   'computation. This can include Python '\n",
              "                                   'REPLs, embeddings, search engines, and '\n",
              "                                   'more. LangChain provides a large '\n",
              "                                   'collection of common utils to use in your '\n",
              "                                   'application. Chains: Chains go beyond just '\n",
              "                                   'a single LLM call, and are sequences of '\n",
              "                                   'calls (whether to an LLM or a different '\n",
              "                                   'utility). LangChain provides a standard '\n",
              "                                   'interface for chains, lots of integrations '\n",
              "                                   'with other tools, and end-to-end chains '\n",
              "                                   'for common applications. Indexes: Language '\n",
              "                                   'models are often more powerful when '\n",
              "                                   'combined with your own text data - this '\n",
              "                                   'module covers best practices for doing '\n",
              "                                   'exactly that. Agents: Agents involve an '\n",
              "                                   'LLM making decisions about which Actions '\n",
              "                                   'to take, taking that Action, seeing an '\n",
              "                                   'Observation, and repeating that until '\n",
              "                                   'done. LangChain provides a standard '\n",
              "                                   'interface for agents, a selection of '\n",
              "                                   'agents to choose from, and examples of end '\n",
              "                                   'to end agents. Memory: Memory is the',\n",
              "                           'url': 'https://langchain.readthedocs.io/en/latest/'},\n",
              "              'score': 0.8426095,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': '353f24c1-f1f0-4aac-b0fa-b6e1f2c5b8cf',\n",
              "              'metadata': {'chunk': 0.0,\n",
              "                           'text': '.rst .pdf Welcome to LangChain Contents '\n",
              "                                   'Getting Started Modules Use Cases '\n",
              "                                   'Reference Docs LangChain Ecosystem '\n",
              "                                   'Additional Resources Welcome to LangChain# '\n",
              "                                   'Large language models (LLMs) are emerging '\n",
              "                                   'as a transformative technology, enabling '\n",
              "                                   'developers to build applications that they '\n",
              "                                   'previously could not. But using these LLMs '\n",
              "                                   'in isolation is often not enough to create '\n",
              "                                   'a truly powerful app - the real power '\n",
              "                                   'comes when you are able to combine them '\n",
              "                                   'with other sources of computation or '\n",
              "                                   'knowledge. This library is aimed at '\n",
              "                                   'assisting in the development of those '\n",
              "                                   'types of applications. Common examples of '\n",
              "                                   'these types of applications include: ‚ùì '\n",
              "                                   'Question Answering over specific documents '\n",
              "                                   'Documentation End-to-end Example: Question '\n",
              "                                   'Answering over Notion Database üí¨ Chatbots '\n",
              "                                   'Documentation End-to-end Example: '\n",
              "                                   'Chat-LangChain ü§ñ Agents Documentation '\n",
              "                                   'End-to-end Example: GPT+WolframAlpha '\n",
              "                                   'Getting Started# Checkout the below guide '\n",
              "                                   'for a walkthrough of how to get started '\n",
              "                                   'using LangChain to create an Language '\n",
              "                                   'Model application. Getting Started '\n",
              "                                   'Documentation Modules# There are several '\n",
              "                                   'main modules that LangChain provides '\n",
              "                                   'support for. For each module we provide '\n",
              "                                   'some examples to get started, how-to '\n",
              "                                   'guides, reference docs, and conceptual '\n",
              "                                   'guides. These modules are, in increasing '\n",
              "                                   'order of complexity: Prompts: This '\n",
              "                                   'includes prompt management, prompt',\n",
              "                           'url': 'https://langchain.readthedocs.io/en/latest/'},\n",
              "              'score': 0.8403774,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': 'e6a8c7ab-857f-46a0-8e3f-ef51e46a9305',\n",
              "              'metadata': {'chunk': 0.0,\n",
              "                           'text': '.ipynb .pdf AI21 AI21# This example goes '\n",
              "                                   'over how to use LangChain to interact with '\n",
              "                                   'AI21 models from langchain.llms import '\n",
              "                                   'AI21 from langchain import PromptTemplate, '\n",
              "                                   'LLMChain template = \"\"\"Question: '\n",
              "                                   \"{question} Answer: Let's think step by \"\n",
              "                                   'step.\"\"\" prompt = '\n",
              "                                   'PromptTemplate(template=template, '\n",
              "                                   'input_variables=[\"question\"]) llm = AI21() '\n",
              "                                   'llm_chain = LLMChain(prompt=prompt, '\n",
              "                                   'llm=llm) question = \"What NFL team won the '\n",
              "                                   'Super Bowl in the year Justin Beiber was '\n",
              "                                   'born?\" llm_chain.run(question) previous '\n",
              "                                   'Integrations next Aleph Alpha By Harrison '\n",
              "                                   'Chase ¬© Copyright 2023, Harrison Chase. '\n",
              "                                   'Last updated on Mar 24, 2023.',\n",
              "                           'url': 'https://langchain.readthedocs.io/en/latest/modules/llms/integrations/ai21.html'},\n",
              "              'score': 0.8263146,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': 'bf44a78f-f1cf-4c45-b7ee-5e7f3aaf46fa',\n",
              "              'metadata': {'chunk': 0.0,\n",
              "                           'text': '.rst .pdf Utility Chains Utility Chains# A '\n",
              "                                   'chain is made up of links, which can be '\n",
              "                                   'either primitives or other chains. '\n",
              "                                   'Primitives can be either prompts, llms, '\n",
              "                                   'utils, or other chains. The examples here '\n",
              "                                   'are all end-to-end chains for specific '\n",
              "                                   'applications, focused on interacting an '\n",
              "                                   'LLMChain with a specific utility. LLMMath '\n",
              "                                   'Links Used: Python REPL, LLMChain Notes: '\n",
              "                                   'This chain takes user input (a math '\n",
              "                                   'question), uses an LLMChain to convert it '\n",
              "                                   'to python code snippet to run in the '\n",
              "                                   'Python REPL, and then returns that as the '\n",
              "                                   'result. Example Notebook PAL Links Used: '\n",
              "                                   'Python REPL, LLMChain Notes: This chain '\n",
              "                                   'takes user input (a reasoning question), '\n",
              "                                   'uses an LLMChain to convert it to python '\n",
              "                                   'code snippet to run in the Python REPL, '\n",
              "                                   'and then returns that as the result. Paper '\n",
              "                                   'Example Notebook SQLDatabase Chain Links '\n",
              "                                   'Used: SQLDatabase, LLMChain Notes: This '\n",
              "                                   'chain takes user input (a question), uses '\n",
              "                                   'a first LLM chain to construct a SQL query '\n",
              "                                   'to run against the SQL database, and then '\n",
              "                                   'uses another LLMChain to take the results '\n",
              "                                   'of that query and use it to answer the '\n",
              "                                   'original question. Example Notebook API '\n",
              "                                   'Chain Links Used: LLMChain, Requests '\n",
              "                                   'Notes: This chain first uses a LLM to '\n",
              "                                   'construct the url to',\n",
              "                           'url': 'https://langchain.readthedocs.io/en/latest/modules/chains/utility_how_to.html'},\n",
              "              'score': 0.8213097,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []}],\n",
              " 'namespace': ''}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBSiDLIUADZ"
      },
      "source": [
        "With retrieval complete, we move on to feeding these into GPT-4 to produce answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfzS4-6-UXgX"
      },
      "source": [
        "## Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPC1jQaKUcy0"
      },
      "source": [
        "GPT-4 is currently accessed via the `ChatCompletions` endpoint of OpenAI. To add the information we retrieved into the model, we need to pass it into our user prompts *alongside* our original query. We can do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "unZstoHNUHeG"
      },
      "outputs": [],
      "source": [
        "# get list of retrieved text\n",
        "contexts = [item['metadata']['text'] for item in res['matches']]\n",
        "\n",
        "augmented_query = \"\\n\\n---\\n\\n\".join(contexts) + \"\\n\\n-----\\n\\n\" + query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRcEHm0Z9fXE",
        "outputId": "872a7f7e-2001-44b5-ff44-cb045365a515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chain first uses a LLM to construct the url to hit, then makes that request with the Requests wrapper, and finally runs that result through the language model again in order to product a natural language response. Example Notebook LLMBash Chain Links Used: BashProcess, LLMChain Notes: This chain takes user input (a question), uses an LLM chain to convert it to a bash command to run in the terminal, and then returns that as the result. Example Notebook LLMChecker Chain Links Used: LLMChain Notes: This chain takes user input (a question), uses an LLM chain to answer that question, and then uses other LLMChains to self-check that answer. Example Notebook LLMRequests Chain Links Used: Requests, LLMChain Notes: This chain takes a URL and other inputs, uses Requests to get the data at that URL, and then passes that along with the other inputs into an LLMChain to generate a response. The example included shows how to ask a question to Google - it firsts constructs a Google url, then fetches the data there, then passes that data + the original question into an LLMChain to get an answer. Example Notebook Moderation Chain Links Used: LLMChain, ModerationChain Notes: This chain shows how to use OpenAI‚Äôs content\n",
            "\n",
            "---\n",
            "\n",
            "Prompts: This includes prompt management, prompt optimization, and prompt serialization. LLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs. Document Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources. Utils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application. Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. Indexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that. Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents. Memory: Memory is the\n",
            "\n",
            "---\n",
            "\n",
            ".rst .pdf Welcome to LangChain Contents Getting Started Modules Use Cases Reference Docs LangChain Ecosystem Additional Resources Welcome to LangChain# Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. But using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you are able to combine them with other sources of computation or knowledge. This library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include: ‚ùì Question Answering over specific documents Documentation End-to-end Example: Question Answering over Notion Database üí¨ Chatbots Documentation End-to-end Example: Chat-LangChain ü§ñ Agents Documentation End-to-end Example: GPT+WolframAlpha Getting Started# Checkout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application. Getting Started Documentation Modules# There are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing order of complexity: Prompts: This includes prompt management, prompt\n",
            "\n",
            "---\n",
            "\n",
            ".ipynb .pdf AI21 AI21# This example goes over how to use LangChain to interact with AI21 models from langchain.llms import AI21 from langchain import PromptTemplate, LLMChain template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\" prompt = PromptTemplate(template=template, input_variables=[\"question\"]) llm = AI21() llm_chain = LLMChain(prompt=prompt, llm=llm) question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\" llm_chain.run(question) previous Integrations next Aleph Alpha By Harrison Chase ¬© Copyright 2023, Harrison Chase. Last updated on Mar 24, 2023.\n",
            "\n",
            "---\n",
            "\n",
            ".rst .pdf Utility Chains Utility Chains# A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, llms, utils, or other chains. The examples here are all end-to-end chains for specific applications, focused on interacting an LLMChain with a specific utility. LLMMath Links Used: Python REPL, LLMChain Notes: This chain takes user input (a math question), uses an LLMChain to convert it to python code snippet to run in the Python REPL, and then returns that as the result. Example Notebook PAL Links Used: Python REPL, LLMChain Notes: This chain takes user input (a reasoning question), uses an LLMChain to convert it to python code snippet to run in the Python REPL, and then returns that as the result. Paper Example Notebook SQLDatabase Chain Links Used: SQLDatabase, LLMChain Notes: This chain takes user input (a question), uses a first LLM chain to construct a SQL query to run against the SQL database, and then uses another LLMChain to take the results of that query and use it to answer the original question. Example Notebook API Chain Links Used: LLMChain, Requests Notes: This chain first uses a LLM to construct the url to\n",
            "\n",
            "-----\n",
            "\n",
            "how do I use the LLMChain in LangChain?\n"
          ]
        }
      ],
      "source": [
        "print(augmented_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sihH_GMiV5_p"
      },
      "source": [
        "Now we ask the question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "IThBqBi8V70d"
      },
      "outputs": [],
      "source": [
        "# system message to 'prime' the model\n",
        "primer = f\"\"\"\n",
        "You are Q&A bot. A highly intelligent system that answers\n",
        "user questions based on the information provided by the user above\n",
        "each question. If the information can not be found in the information\n",
        "provided by the user you truthfully say \"I don't know\".\n",
        "\"\"\".strip()\n",
        "\n",
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvS1yJhOWpiJ"
      },
      "source": [
        "To display this response nicely, we will display it in markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "RDo2qeMHWto1",
        "outputId": "26d30abb-767a-4256-cd48-50af20128c84"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To use the LLMChain in LangChain, you need to follow these steps:\n",
              "\n",
              "1. Import the required libraries and classes:\n",
              "\n",
              "```python\n",
              "from langchain import PromptTemplate, LLMChain\n",
              "from langchain.llms import AI21  # Or any other LLM you want to use\n",
              "```\n",
              "\n",
              "2. Define a PromptTemplate:\n",
              "\n",
              "```python\n",
              "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
              "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
              "```\n",
              "\n",
              "3. Create an instance of the LLM you want to use (e.g., AI21):\n",
              "\n",
              "```python\n",
              "llm = AI21()\n",
              "```\n",
              "\n",
              "4. Create an LLMChain with the defined prompt and LLM instance:\n",
              "\n",
              "```python\n",
              "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
              "```\n",
              "\n",
              "5. Run the LLMChain with a question as input:\n",
              "\n",
              "```python\n",
              "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
              "response = llm_chain.run(question)\n",
              "```\n",
              "\n",
              "In this example, the LLMChain combines the PromptTemplate, LLM, and input question to generate a response using the AI21 large language model. You can customize the template and use different LLMs based on your requirements."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ-a8MHg0eYQ"
      },
      "source": [
        "Let's compare this to a non-augmented query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "vwhaSgdF0ZDX",
        "outputId": "7a139208-bc40-4784-fae8-a10caf09800e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't know."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSsA-dW0m_P"
      },
      "source": [
        "If we drop the `\"I don't know\"` part of the `primer`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "Z3svdTCZ0iJ2",
        "outputId": "e6a8c0bf-e575-454c-bd20-e3b1db3e47b0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain is not a well-known technology, and there isn't enough information available for a specific term like \"LLMChain.\" Therefore, it's not possible for me to provide you with accurate guidance on how to use the LLMChain in LangChain.\n",
              "\n",
              "However, if you're referring to a custom library or framework, your best course of action would be to consult the developer who created the technology or refer to the documentation provided for that specific library or framework."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqDEXo3c0w1K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04fd6e9cebaa4c9287d16cb8c861c8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "144e2e3c8a014c549e0f552a64a670ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "153f898146264d50b77b5ef23db92408": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bed5d4ebf054e80b4d63d6f8a2593d8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_04fd6e9cebaa4c9287d16cb8c861c8a3",
            "value": " 231/231 [00:01&lt;00:00, 193.80it/s]"
          }
        },
        "157f79e1ecf0423393cb15dcd2e66996": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b93eb9d358041ab99fe87045f7f0660": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bed5d4ebf054e80b4d63d6f8a2593d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30d7236e54844058b7c76404e1f2ccb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "348fe1bd02ed4dca8df422031d1184f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70eaf7d1a5b24e49a32490fd3a75ea15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9552f4dca1642e2924ee152067f1f3d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c82f8fbcef0648489f1dcbb4af5ea8c4",
            "value": " 12/12 [00:18&lt;00:00,  3.12s/it]"
          }
        },
        "7d78613ce91b4427a4afacb699ef031e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "865d0f1e70cc4889aaf91cf1ad82b909": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f0edead487948358efe478a01316209": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_348fe1bd02ed4dca8df422031d1184f6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_157f79e1ecf0423393cb15dcd2e66996",
            "value": "100%"
          }
        },
        "90aa35cbdf0c45a1bb7b9075c48a6f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b93eb9d358041ab99fe87045f7f0660",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af4f336bfcb446afb9e6a513d49d791f",
            "value": 12
          }
        },
        "a43a7db5cab149b3a8aeef23d3fb936f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9552f4dca1642e2924ee152067f1f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af4f336bfcb446afb9e6a513d49d791f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1489d5d6c1f498fadaea8aeb16ab60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5b49411f2134a9b9649528314f746d6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7d78613ce91b4427a4afacb699ef031e",
            "value": "100%"
          }
        },
        "b6b5865b02504e10a020ad5f42241df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1489d5d6c1f498fadaea8aeb16ab60f",
              "IPY_MODEL_90aa35cbdf0c45a1bb7b9075c48a6f7d",
              "IPY_MODEL_70eaf7d1a5b24e49a32490fd3a75ea15"
            ],
            "layout": "IPY_MODEL_144e2e3c8a014c549e0f552a64a670ef"
          }
        },
        "bcd3e274345a44dc950890c1fa1026a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a43a7db5cab149b3a8aeef23d3fb936f",
            "max": 231,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_865d0f1e70cc4889aaf91cf1ad82b909",
            "value": 231
          }
        },
        "c82f8fbcef0648489f1dcbb4af5ea8c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5b49411f2134a9b9649528314f746d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe4aa5160ef74ecd820f9c6f7de035a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f0edead487948358efe478a01316209",
              "IPY_MODEL_bcd3e274345a44dc950890c1fa1026a7",
              "IPY_MODEL_153f898146264d50b77b5ef23db92408"
            ],
            "layout": "IPY_MODEL_30d7236e54844058b7c76404e1f2ccb8"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
