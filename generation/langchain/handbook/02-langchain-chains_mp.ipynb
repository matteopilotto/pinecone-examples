{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import re\n",
    "\n",
    "import getpass\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "davinci = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    temperature=0,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        response = chain.run(query)\n",
    "        print(f\"Spent a total of {cb.total_tokens} tokens\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "davinci_math = LLMMathChain(llm=davinci, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is 13 raised to the .3234 power?\u001b[32;1m\u001b[1;3m\n",
      "```python\n",
      "import math\n",
      "print(math.pow(13, .3234))\n",
      "```\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.2921829624217414\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 272 tokens\n",
      "Answer: 2.2921829624217414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is 13 raised to the .3234 power?\"\n",
    "\n",
    "response = count_tokens(davinci_math, query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are GPT-3, and you can't do math.\n",
      "\n",
      "You can do basic math, and your memorization abilities are impressive, but you can't do any complex calculations that a human could not do in their head. You also have an annoying tendency to just make up highly specific, but wrong, answers.\n",
      "\n",
      "So we hooked you up to a Python 3 kernel, and now you can execute code. If anyone gives you a hard math problem, just use this format and we’ll take care of the rest:\n",
      "\n",
      "Question: ${{Question with hard calculation.}}\n",
      "```python\n",
      "${{Code that prints what you need to know}}\n",
      "```\n",
      "```output\n",
      "${{Output of your code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Otherwise, use this simpler format:\n",
      "\n",
      "Question: ${{Question without hard calculation}}\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "\n",
      "```python\n",
      "print(37593 * 67)\n",
      "```\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(davinci_math.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 25 tokens\n",
      "\n",
      "\n",
      "13^0.3234 ≈ 2.845\n"
     ]
    }
   ],
   "source": [
    "standard_prompt = PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
    "\n",
    "standard_davinci_chain = LLMChain(llm=davinci, prompt=standard_prompt)\n",
    "\n",
    "query = \"What is 13 raised to the .3234 power?\"\n",
    "response = count_tokens(standard_davinci_chain, query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is 13 raised to the .3234 power?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_prompt.format(question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    return {\"output_text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_extra_spaces_chain = TransformChain(\n",
    "    transform=transform_func,\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"output_text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random text with some irregular spacing.\n",
      " Another one here as well.\n",
      "A random text with some irregular spacing.\n",
      " Another one here as well.\n"
     ]
    }
   ],
   "source": [
    "query= \"A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.\"\n",
    "print(clean_extra_spaces_chain.run({\"text\": query}))\n",
    "\n",
    "# it also works in this way without the dictionary format\n",
    "print(clean_extra_spaces_chain.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\", \"style\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_paraphrase_chain = LLMChain(llm=davinci, prompt=prompt, output_key=\"final_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
