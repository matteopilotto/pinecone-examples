{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
    "                                                  ConversationSummaryMemory,\n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "davinci = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        response = chain.run(query)\n",
    "        print(f\"Spend a total of {cb.total_tokens} tokens.\\n\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationChain(llm=davinci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    return text.strip().replace(\". \", \".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI.\n",
      "The AI is talkative and provides lots of specific details from its context.\n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(format_text(conversation_chain.prompt.template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_conv = [\n",
    "    \"Good morning AI\",\n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\",\n",
    "    \"I just want to analyze the different possibilities. What can you think of?\",\n",
    "    \"Which data source types could be used to give context to the model?\",\n",
    "    \"What is my aim again?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_buffer_chain = ConversationChain(\n",
    "    llm=davinci,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'Good morning AI'\n",
      "\n",
      "{'input': 'Good morning AI', 'history': '', 'response': \" Good morning! It's a beautiful day today, isn't it? The sun is shining and the birds are singing. How can I help you?\"}\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[0]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = conv_buffer_chain(query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'Good morning AI'\n",
      "\n",
      "Spend a total of 94 tokens.\n",
      "\n",
      "Good morning! It's a beautiful day today, isn't it? The sun is shining and the birds are singing.\n",
      "How can I help you?\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[0]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buffer_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'My interest here is to explore the potential of integrating Large Language Models with external knowledge'\n",
      "\n",
      "Spend a total of 186 tokens.\n",
      "\n",
      "Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text.\n",
      "They can be used to generate text from a given context, or to answer questions about a given topic.\n",
      "Integrating them with external knowledge can help them become more accurate and provide more detailed answers.\n",
      "What kind of external knowledge are you looking to integrate?\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[1]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buffer_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'I just want to analyze the different possibilities. What can you think of?'\n",
      "\n",
      "Spend a total of 294 tokens.\n",
      "\n",
      "Well, one possibility is to integrate external knowledge sources such as databases, ontologies, and other structured data.\n",
      "This can help the Large Language Model better understand the context of the text and provide more accurate answers.\n",
      "Another possibility is to use external knowledge to generate more detailed and accurate text.\n",
      "For example, a Large Language Model could be trained on a large corpus of text and then use external knowledge to generate more detailed and accurate text.\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[2]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buffer_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'Which data source types could be used to give context to the model?'\n",
      "\n",
      "Spend a total of 370 tokens.\n",
      "\n",
      "There are a variety of data sources that can be used to give context to a Large Language Model.\n",
      "These include databases, ontologies, and other structured data.\n",
      "Additionally, unstructured data such as text, images, and audio can also be used to provide context.\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[3]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buffer_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'What is my aim again?'\n",
      "\n",
      "Spend a total of 457 tokens.\n",
      "\n",
      "Your aim is to explore the potential of integrating Large Language Models with external knowledge.\n",
      "This could include integrating external knowledge sources such as databases, ontologies, and other structured data to help the Large Language Model better understand the context of the text and provide more accurate answers.\n",
      "Additionally, unstructured data such as text, images, and audio can also be used to provide context.\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[4]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buffer_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI\n",
      "AI:  Good morning! It's a beautiful day today, isn't it? The sun is shining and the birds are singing. How can I help you?\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI:  Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given topic. Integrating them with external knowledge can help them become more accurate and provide more detailed answers. What kind of external knowledge are you looking to integrate?\n",
      "Human: I just want to analyze the different possibilities. What can you think of?\n",
      "AI:  Well, one possibility is to integrate external knowledge sources such as databases, ontologies, and other structured data. This can help the Large Language Model better understand the context of the text and provide more accurate answers. Another possibility is to use external knowledge to generate more detailed and accurate text. For example, a Large Language Model could be trained on a large corpus of text and then use external knowledge to generate more detailed and accurate text.\n",
      "Human: Which data source types could be used to give context to the model?\n",
      "AI:   There are a variety of data sources that can be used to give context to a Large Language Model. These include databases, ontologies, and other structured data. Additionally, unstructured data such as text, images, and audio can also be used to provide context.\n",
      "Human: What is my aim again?\n",
      "AI:  Your aim is to explore the potential of integrating Large Language Models with external knowledge. This could include integrating external knowledge sources such as databases, ontologies, and other structured data to help the Large Language Model better understand the context of the text and provide more accurate answers. Additionally, unstructured data such as text, images, and audio can also be used to provide context.\n"
     ]
    }
   ],
   "source": [
    "print(conv_buffer_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_summary_chain = ConversationChain(\n",
    "    llm=davinci,\n",
    "    memory=ConversationSummaryMemory(llm=davinci)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI.\n",
      "The AI is talkative and provides lots of specific details from its context.\n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(format_text(conv_summary_chain.prompt.template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence.\n",
      "The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence.\n",
      "The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(format_text(conv_summary_chain.memory.prompt.template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'Good morning AI'\n",
      "\n",
      "Spend a total of 308 tokens.\n",
      "\n",
      "Good morning! It's a beautiful day today, isn't it? The sun is shining and the birds are singing.\n",
      "How can I help you?\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[0]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_summary_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'My interest here is to explore the potential of integrating Large Language Models with external knowledge'\n",
      "\n",
      "Spend a total of 441 tokens.\n",
      "\n",
      "That sounds like an interesting project! I'm familiar with Large Language Models, but I'm not sure how they could be integrated with external knowledge.\n",
      "Could you tell me more about what you have in mind?\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[1]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_summary_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'I just want to analyze the different possibilities. What can you think of?'\n",
      "\n",
      "Spend a total of 660 tokens.\n",
      "\n",
      "I can think of a few possibilities.\n",
      "One option is to use a large language model to generate a set of candidate answers to a given query, and then use external knowledge to filter out the most relevant answers.\n",
      "Another option is to use the large language model to generate a set of candidate answers, and then use external knowledge to score and rank the answers.\n",
      "Finally, you could use the large language model to generate a set of candidate answers, and then use external knowledge to refine the answers.\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[2]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_summary_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'Which data source types could be used to give context to the model?'\n",
      "\n",
      "Spend a total of 774 tokens.\n",
      "\n",
      "There are many different types of data sources that could be used to give context to the model.\n",
      "These include structured data sources such as databases, unstructured data sources such as text documents, and semi-structured data sources such as webpages.\n",
      "Additionally, you could use external knowledge sources such as ontologies, taxonomies, and thesauri.\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[3]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_summary_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'What is my aim again?'\n",
      "\n",
      "Spend a total of 826 tokens.\n",
      "\n",
      "Your aim is to explore the potential of integrating Large Language Models with external knowledge.\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[4]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_summary_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human greeted the AI with a good morning, to which the AI responded with a cheerful greeting and asked how it could help.\n",
      "The human then expressed interest in exploring the potential of integrating Large Language Models with external knowledge, to which the AI responded positively and asked for more information.\n",
      "The AI then suggested a few possibilities for using the large language model and external knowledge, such as using it to generate a set of candidate answers and filter out the most relevant ones, score and rank the answers, or refine the answers.\n",
      "The human then asked which data source types could be used to give context to the model, to which the AI responded that there are many different types of data sources that could be used, such as structured data sources, unstructured data sources, semi-structured data sources, and external knowledge sources such as ontologies, taxonomies, and thesauri.\n",
      "The human then asked what their aim was again, to which the AI responded that their aim was to explore the potential of integrating Large Language Models with external knowledge.\n"
     ]
    }
   ],
   "source": [
    "print(format_text(conv_summary_chain.memory.buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 403\n",
      "Summary memory conversation length: 210\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "\n",
    "print(f\"Buffer memory conversation length: {len(tokenizer.encode(conv_buffer_chain.memory.buffer))}\")\n",
    "print(f\"Summary memory conversation length: {len(tokenizer.encode(conv_summary_chain.memory.buffer))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation buffer window memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_buf_window_chain = ConversationChain(\n",
    "    llm=davinci,\n",
    "    memory=ConversationBufferWindowMemory(k=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'Good morning AI'\n",
      "\n",
      "Spend a total of 94 tokens.\n",
      "\n",
      "Good morning! It's a beautiful day today, isn't it? The sun is shining and the birds are singing.\n",
      "How can I help you?\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[0]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buf_window_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'My interest here is to explore the potential of integrating Large Language Models with external knowledge'\n",
      "\n",
      "Spend a total of 186 tokens.\n",
      "\n",
      "Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text.\n",
      "They can be used to generate text from a given context, or to answer questions about a given topic.\n",
      "Integrating them with external knowledge can help them become more accurate and provide more detailed answers.\n",
      "What kind of external knowledge are you looking to integrate?\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[1]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buf_window_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'I just want to analyze the different possibilities. What can you think of?'\n",
      "\n",
      "Spend a total of 294 tokens.\n",
      "\n",
      "Well, one possibility is to integrate external knowledge sources such as databases, ontologies, and other structured data.\n",
      "This can help the Large Language Model better understand the context of the text and provide more accurate answers.\n",
      "Another possibility is to use external knowledge to generate more detailed and accurate text.\n",
      "For example, a Large Language Model could be trained on a large corpus of text and then use external knowledge to generate more detailed and accurate text.\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[2]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buf_window_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'Which data source types could be used to give context to the model?'\n",
      "\n",
      "Spend a total of 362 tokens.\n",
      "\n",
      "There are a variety of data sources that could be used to give context to the model.\n",
      "These include databases, ontologies, and other structured data.\n",
      "Additionally, unstructured data such as text, audio, and video can also be used to provide context.\n",
      "For example, a Large Language Model could be trained on a large corpus of text and then use external knowledge from audio or video sources to generate more detailed and accurate text.\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[3]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buf_window_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER INPUT: 'What is my aim again?'\n",
      "\n",
      "Spend a total of 304 tokens.\n",
      "\n",
      "Your aim is to analyze the different possibilities for using external knowledge sources to improve the accuracy of a Large Language Model.\n"
     ]
    }
   ],
   "source": [
    "query = full_conv[4]\n",
    "\n",
    "print(f\"USER INPUT: '{query}'\\n\")\n",
    "\n",
    "response = get_response(conv_buf_window_chain, query)\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Which data source types could be used to give context to the model?\n",
      "AI:   There are a variety of data sources that could be used to give context to the model.\n",
      "These include databases, ontologies, and other structured data.\n",
      "Additionally, unstructured data such as text, audio, and video can also be used to provide context.\n",
      "For example, a Large Language Model could be trained on a large corpus of text and then use external knowledge from audio or video sources to generate more detailed and accurate text.\n",
      "Human: What is my aim again?\n",
      "AI:  Your aim is to analyze the different possibilities for using external knowledge sources to improve the accuracy of a Large Language Model.\n"
     ]
    }
   ],
   "source": [
    "print(format_text(conv_buf_window_chain.memory.load_memory_variables(inputs=[])[\"history\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Knowledge Graph memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kg_chain = ConversationChain(\n",
    "    llm=davinci,\n",
    "    memory=ConversationKGMemory(llm=davinci)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spend a total of 1150 tokens.\n",
      "\n",
      "Hi Human! My name is AI.\n",
      "It's nice to meet you.\n",
      "I'm not familiar with magoes, what are they?\n"
     ]
    }
   ],
   "source": [
    "query = \"My name is Human and I like magoes!\"\n",
    "\n",
    "response = get_response(conv_kg_chain, query)\n",
    "\n",
    "print(format_text(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Human', 'Human', 'name is'), ('Human', 'magoes', 'likes')]\n"
     ]
    }
   ],
   "source": [
    "print(conv_kg_chain.memory.kg.get_triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
