{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/xx-langchain-chunking.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/langchain/handbook/xx-langchain-chunking.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
    "\n",
    "# Preparing Text Data for use with Retrieval-Augmented LLMs\n",
    "\n",
    "In this walkthrough we'll take a look at an example and some of the considerations when we need to prepare text data for retrieval augmented question-answering using **L**arge **L**anguage **M**odels (LLMs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few Python libraries we must `pip install` for this notebook to run, those are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain tiktoken matplotlib seaborn tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will download the LangChain docs from [langchain.readthedocs.io/](https://langchain.readthedocs.io/latest/en/). We get all `.html` files located on the site like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -r -A.html -P rtdocs https://langchain.readthedocs.io/en/latest/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This downloads all HTML into the `rtdocs` directory. Now we can use LangChain itself to process these docs. We do this using the `ReadTheDocsLoader` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matte\\miniconda3\\envs\\ml\\lib\\site-packages\\langchain\\document_loaders\\readthedocs.py:30: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 30 of the file c:\\Users\\matte\\miniconda3\\envs\\ml\\lib\\site-packages\\langchain\\document_loaders\\readthedocs.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  _ = BeautifulSoup(\n",
      "c:\\Users\\matte\\miniconda3\\envs\\ml\\lib\\site-packages\\langchain\\document_loaders\\readthedocs.py:46: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 46 of the file c:\\Users\\matte\\miniconda3\\envs\\ml\\lib\\site-packages\\langchain\\document_loaders\\readthedocs.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(data, **self.bs_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "\n",
    "loader = ReadTheDocsLoader('rtdocs')\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with `389` processed doc pages. Let's take a look at the format each one contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='.md\\n.pdf\\nDeployments\\n Contents \\nStreamlit\\nGradio (on Hugging Face)\\nBeam\\nVercel\\nSteamShip\\nDeployments#\\nSo you‚Äôve made a really cool chain - now what? How do you deploy it and make it easily sharable with the world?\\nThis section covers several options for that.\\nNote that these are meant as quick deployment options for prototypes and demos, and not for production systems.\\nIf you are looking for help with deployment of a production system, please contact us directly.\\nWhat follows is a list of template GitHub repositories aimed that are intended to be\\nvery easy to fork and modify to use your chain.\\nThis is far from an exhaustive list of options, and we are EXTREMELY open to contributions here.\\nStreamlit#\\nThis repo serves as a template for how to deploy a LangChain with Streamlit.\\nIt implements a chatbot interface.\\nIt also contains instructions for how to deploy this app on the Streamlit platform.\\nGradio (on Hugging Face)#\\nThis repo serves as a template for how deploy a LangChain with Gradio.\\nIt implements a chatbot interface, with a ‚ÄúBring-Your-Own-Token‚Äù approach (nice for not wracking up big bills).\\nIt also contains instructions for how to deploy this app on the Hugging Face platform.\\nThis is heavily influenced by James Weaver‚Äôs excellent examples.\\nBeam#\\nThis repo serves as a template for how deploy a LangChain with Beam.\\nIt implements a Question Answering app and contains instructions for deploying the app as a serverless REST API.\\nVercel#\\nA minimal example on how to run LangChain on Vercel using Flask.\\nSteamShip#\\nThis repository contains LangChain adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship.\\nThis includes: production ready endpoints, horizontal scaling across dependencies, persistant storage of app state, multi-tenancy support, etc.\\nprevious\\nLangChain Gallery\\nnext\\nTracing\\n Contents\\n  \\nStreamlit\\nGradio (on Hugging Face)\\nBeam\\nVercel\\nSteamShip\\nBy Harrison Chase\\n    \\n      ¬© Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Mar 24, 2023.\\n  ', lookup_str='', metadata={'source': 'rtdocs\\\\langchain.readthedocs.io\\\\en\\\\latest\\\\deployments.html'}, lookup_index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the plaintext page content like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".md\n",
      ".pdf\n",
      "Deployments\n",
      " Contents \n",
      "Streamlit\n",
      "Gradio (on Hugging Face)\n",
      "Beam\n",
      "Vercel\n",
      "SteamShip\n",
      "Deployments#\n",
      "So you‚Äôve made a really cool chain - now what? How do you deploy it and make it easily sharable with the world?\n",
      "This section covers several options for that.\n",
      "Note that these are meant as quick deployment options for prototypes and demos, and not for production systems.\n",
      "If you are looking for help with deployment of a production system, please contact us directly.\n",
      "What follows is a list of template GitHub repositories aimed that are intended to be\n",
      "very easy to fork and modify to use your chain.\n",
      "This is far from an exhaustive list of options, and we are EXTREMELY open to contributions here.\n",
      "Streamlit#\n",
      "This repo serves as a template for how to deploy a LangChain with Streamlit.\n",
      "It implements a chatbot interface.\n",
      "It also contains instructions for how to deploy this app on the Streamlit platform.\n",
      "Gradio (on Hugging Face)#\n",
      "This repo serves as a template for how deploy a LangChain with Gradio.\n",
      "It implements a chatbot interface, with a ‚ÄúBring-Your-Own-Token‚Äù approach (nice for not wracking up big bills).\n",
      "It also contains instructions for how to deploy this app on the Hugging Face platform.\n",
      "This is heavily influenced by James Weaver‚Äôs excellent examples.\n",
      "Beam#\n",
      "This repo serves as a template for how deploy a LangChain with Beam.\n",
      "It implements a Question Answering app and contains instructions for deploying the app as a serverless REST API.\n",
      "Vercel#\n",
      "A minimal example on how to run LangChain on Vercel using Flask.\n",
      "SteamShip#\n",
      "This repository contains LangChain adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship.\n",
      "This includes: production ready endpoints, horizontal scaling across dependencies, persistant storage of app state, multi-tenancy support, etc.\n",
      "previous\n",
      "LangChain Gallery\n",
      "next\n",
      "Tracing\n",
      " Contents\n",
      "  \n",
      "Streamlit\n",
      "Gradio (on Hugging Face)\n",
      "Beam\n",
      "Vercel\n",
      "SteamShip\n",
      "By Harrison Chase\n",
      "    \n",
      "      ¬© Copyright 2023, Harrison Chase.\n",
      "      \n",
      "  Last updated on Mar 24, 2023.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".rst\n",
      ".pdf\n",
      "Welcome to LangChain\n",
      " Contents \n",
      "Getting Started\n",
      "Modules\n",
      "Use Cases\n",
      "Reference Docs\n",
      "LangChain Ecosystem\n",
      "Additional Resources\n",
      "Welcome to LangChain#\n",
      "Large language models (LLMs) are emerging as a transformative technology, enabling\n",
      "developers to build applications that they previously could not.\n",
      "But using these LLMs in isolation is often not enough to\n",
      "create a truly powerful app - the real power comes when you are able to\n",
      "combine them with other sources of computation or knowledge.\n",
      "This library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\n",
      "‚ùì Question Answering over specific documents\n",
      "Documentation\n",
      "End-to-end Example: Question Answering over Notion Database\n",
      "üí¨ Chatbots\n",
      "Documentation\n",
      "End-to-end Example: Chat-LangChain\n",
      "ü§ñ Agents\n",
      "Documentation\n",
      "End-to-end Example: GPT+WolframAlpha\n",
      "Getting Started#\n",
      "Checkout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\n",
      "Getting Started Documentation\n",
      "Modules#\n",
      "There are several main modules that LangChain provides support for.\n",
      "For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\n",
      "These modules are, in increasing order of complexity:\n",
      "Prompts: This includes prompt management, prompt optimization, and prompt serialization.\n",
      "LLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs.\n",
      "Document Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources.\n",
      "Utils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application.\n",
      "Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
      "Indexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\n",
      "Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\n",
      "Memory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
      "Chat: Chat models are a variation on Language Models that expose a different API - rather than working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above.\n",
      "Use Cases#\n",
      "The above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\n",
      "Agents: Agents are systems that use a language model to interact with other tools. These can be used to do more grounded question/answering, interact with APIs, or even take actions.\n",
      "Chatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\n",
      "Data Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\n",
      "Question Answering: Answering questions over specific documents, only utilizing the information in those documents to construct an answer. A type of Data Augmented Generation.\n",
      "Summarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\n",
      "Querying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\n",
      "Evaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\n",
      "Generate similar examples: Generating similar examples to a given input. This is a common use case for many applications, and LangChain provides some prompts/chains for assisting in this.\n",
      "Compare models: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\n",
      "Reference Docs#\n",
      "All of LangChain‚Äôs reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\n",
      "Reference Documentation\n",
      "LangChain Ecosystem#\n",
      "Guides for how other companies/products can be used with LangChain\n",
      "LangChain Ecosystem\n",
      "Additional Resources#\n",
      "Additional collection of resources we think may be useful as you develop your application!\n",
      "LangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\n",
      "Glossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\n",
      "Gallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\n",
      "Deployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\n",
      "Discord: Join us on our Discord to discuss all things LangChain!\n",
      "Tracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\n",
      "Production Support: As you move your LangChains into production, we‚Äôd love to offer more comprehensive support. Please fill out this form and we‚Äôll set up a dedicated support Slack channel.\n",
      "next\n",
      "Quickstart Guide\n",
      " Contents\n",
      "  \n",
      "Getting Started\n",
      "Modules\n",
      "Use Cases\n",
      "Reference Docs\n",
      "LangChain Ecosystem\n",
      "Additional Resources\n",
      "By Harrison Chase\n",
      "    \n",
      "      ¬© Copyright 2023, Harrison Chase.\n",
      "      \n",
      "  Last updated on Mar 24, 2023.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(docs[5].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the source of each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://langchain.readthedocs.io\\\\en\\\\latest\\\\index.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docs[5].metadata['source'].replace('rtdocs/', 'https://')\n",
    "docs[5].metadata['source'].replace('rtdocs\\\\', 'https://')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, we need to also consider the length of each page with respect to the number of tokens that will reasonably fit within the window of the latest LLMs. We will use `gpt-3.5-turbo` as an example.\n",
    "\n",
    "To count the number of tokens that `gpt-3.5-turbo` will use for some text we need to initialize the `tiktoken` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "\n",
    "    return len(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the tokenizer we defined the encoder as `\"cl100k_base\"`. This is a specific tiktoken encoder which is used by `gpt-3.5-turbo`. Other encoders exist. At the time of writing the OpenAI specific tokenizers (using `tiktoken`) are summarized as:\n",
    "\n",
    "| Encoder | Models |\n",
    "| --- | --- |\n",
    "| `cl100k_base` | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002` |\n",
    "| `p50k_base` | `text-davinci-003`, `code-davinci-002`, `code-cushman-002` |\n",
    "| `r50k_base` | `text-davinci-001`, `davinci`, `text-similarity-davinci-001` |\n",
    "| `gpt2` | `gpt2` |\n",
    "\n",
    "You can find these details in the [Tiktoken `model.py` script](https://github.com/openai/tiktoken/blob/main/tiktoken/model.py), or using `tiktoken.encoding_for_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `tiktoken_len` function, let's count and visualize the number of tokens across our webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = [tiktoken_len(doc.page_content) for doc in docs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see `min`, average, and `max` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 45\n",
      "Avg: 1329\n",
      "Max: 57674\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Min: {min(token_counts)}\n",
    "Avg: {int(sum(token_counts) / len(token_counts))}\n",
    "Max: {max(token_counts)}\n",
    "\"\"\".strip()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set style and color palette for the plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"muted\")\n",
    "\n",
    "# create histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(token_counts, kde=False, bins=50)\n",
    "\n",
    "# customize the plot info\n",
    "plt.title(\"Token Counts Histogram\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of pages seem to contain a lower number of tokens. But our limits for the number of tokens to add to each chunk is actually smaller than some of the smaller pages. But, how do we decide what this number should be?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the Text\n",
    "\n",
    "At the time of writing, `gpt-3.5-turbo` supports a context window of 4096 tokens ‚Äî that means that input tokens + generated ( / completion) output tokens, cannot total more than 4096 without hitting an error.\n",
    "\n",
    "So we 100% need to keep below this. If we assume a very safe margin of ~2000 tokens for the input prompt into `gpt-3.5-turbo`, leaving ~2000 tokens for conversation history and completion.\n",
    "\n",
    "With this ~2000 token limit we may want to include *five* snippets of relevant information, meaning each snippet can be no more than **400** token long.\n",
    "\n",
    "To create these snippets we use the `RecursiveCharacterTextSplitter` from LangChain. To measure the length of snippets we also need a *length function*. This is a function that consumes text, counts the number of tokens within the text (after tokenization using the `gpt-3.5-turbo` tokenizer), and returns that number. We define it like so:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the length function defined we can initialize our `RecursiveCharacterTextSplitter` object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the text for a document like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(docs[5].page_content)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 375\n",
      "294 375\n",
      "254 368\n",
      "106 147\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(len(chunk.split(' ')), tiktoken_len(chunk))\n",
    "# tiktoken_len(chunks[0]), tiktoken_len(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('page_content',\n",
       "  '.md\\n.pdf\\nDeployments\\n Contents \\nStreamlit\\nGradio (on Hugging Face)\\nBeam\\nVercel\\nSteamShip\\nDeployments#\\nSo you‚Äôve made a really cool chain - now what? How do you deploy it and make it easily sharable with the world?\\nThis section covers several options for that.\\nNote that these are meant as quick deployment options for prototypes and demos, and not for production systems.\\nIf you are looking for help with deployment of a production system, please contact us directly.\\nWhat follows is a list of template GitHub repositories aimed that are intended to be\\nvery easy to fork and modify to use your chain.\\nThis is far from an exhaustive list of options, and we are EXTREMELY open to contributions here.\\nStreamlit#\\nThis repo serves as a template for how to deploy a LangChain with Streamlit.\\nIt implements a chatbot interface.\\nIt also contains instructions for how to deploy this app on the Streamlit platform.\\nGradio (on Hugging Face)#\\nThis repo serves as a template for how deploy a LangChain with Gradio.\\nIt implements a chatbot interface, with a ‚ÄúBring-Your-Own-Token‚Äù approach (nice for not wracking up big bills).\\nIt also contains instructions for how to deploy this app on the Hugging Face platform.\\nThis is heavily influenced by James Weaver‚Äôs excellent examples.\\nBeam#\\nThis repo serves as a template for how deploy a LangChain with Beam.\\nIt implements a Question Answering app and contains instructions for deploying the app as a serverless REST API.\\nVercel#\\nA minimal example on how to run LangChain on Vercel using Flask.\\nSteamShip#\\nThis repository contains LangChain adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship.\\nThis includes: production ready endpoints, horizontal scaling across dependencies, persistant storage of app state, multi-tenancy support, etc.\\nprevious\\nLangChain Gallery\\nnext\\nTracing\\n Contents\\n  \\nStreamlit\\nGradio (on Hugging Face)\\nBeam\\nVercel\\nSteamShip\\nBy Harrison Chase\\n    \\n      ¬© Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Mar 24, 2023.\\n  '),\n",
       " ('lookup_str', ''),\n",
       " ('metadata',\n",
       "  {'source': 'rtdocs\\\\langchain.readthedocs.io\\\\en\\\\latest\\\\deployments.html'}),\n",
       " ('lookup_index', 0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(docs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `docs[5]` we created `2` chunks of token length `346` and `247`.\n",
    "\n",
    "This is for a single document, we need to do this over all of our documents. While we iterate through the docs to create these chunks we will reformat them into a format that looks like:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"abc-0\",\n",
    "        \"text\": \"some important document text\",\n",
    "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"abc-1\",\n",
    "        \"text\": \"the next chunk of important document text\",\n",
    "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
    "    }\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"id\"` will be created based on the URL of the text + it's chunk number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://langchain.readthedocs.io\\en\\latest\\index.html\n",
      "30b694fe53d5\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "m = hashlib.md5()  # this will convert URL into unique ID\n",
    "\n",
    "url = docs[5].metadata['source'].replace('rtdocs\\\\', 'https://')\n",
    "print(url)\n",
    "\n",
    "# convert URL to unique ID\n",
    "m.update(url.encode('utf-8'))\n",
    "uid = m.hexdigest()[:12]\n",
    "print(uid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the `uid` alongside chunk number and actual `url` to create the format needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '30b694fe53d5-0',\n",
       "  'text': '.rst\\n.pdf\\nWelcome to LangChain\\n Contents \\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\nWelcome to LangChain#\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\ndevelopers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to\\ncreate a truly powerful app - the real power comes when you are able to\\ncombine them with other sources of computation or knowledge.\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\n‚ùì Question Answering over specific documents\\nDocumentation\\nEnd-to-end Example: Question Answering over Notion Database\\nüí¨ Chatbots\\nDocumentation\\nEnd-to-end Example: Chat-LangChain\\nü§ñ Agents\\nDocumentation\\nEnd-to-end Example: GPT+WolframAlpha\\nGetting Started#\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\nGetting Started Documentation\\nModules#\\nThere are several main modules that LangChain provides support for.\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\nThese modules are, in increasing order of complexity:\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\nLLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs.\\nDocument Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources.\\nUtils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application.',\n",
       "  'source': 'https://langchain.readthedocs.io\\\\en\\\\latest\\\\index.html'},\n",
       " {'id': '30b694fe53d5-1',\n",
       "  'text': 'Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\nChat: Chat models are a variation on Language Models that expose a different API - rather than working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above.\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\nAgents: Agents are systems that use a language model to interact with other tools. These can be used to do more grounded question/answering, interact with APIs, or even take actions.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nData Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.',\n",
       "  'source': 'https://langchain.readthedocs.io\\\\en\\\\latest\\\\index.html'},\n",
       " {'id': '30b694fe53d5-2',\n",
       "  'text': 'Question Answering: Answering questions over specific documents, only utilizing the information in those documents to construct an answer. A type of Data Augmented Generation.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\nGenerate similar examples: Generating similar examples to a given input. This is a common use case for many applications, and LangChain provides some prompts/chains for assisting in this.\\nCompare models: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\nReference Docs#\\nAll of LangChain‚Äôs reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\nReference Documentation\\nLangChain Ecosystem#\\nGuides for how other companies/products can be used with LangChain\\nLangChain Ecosystem\\nAdditional Resources#\\nAdditional collection of resources we think may be useful as you develop your application!\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.',\n",
       "  'source': 'https://langchain.readthedocs.io\\\\en\\\\latest\\\\index.html'},\n",
       " {'id': '30b694fe53d5-3',\n",
       "  'text': 'Deployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\nDiscord: Join us on our Discord to discuss all things LangChain!\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\nProduction Support: As you move your LangChains into production, we‚Äôd love to offer more comprehensive support. Please fill out this form and we‚Äôll set up a dedicated support Slack channel.\\nnext\\nQuickstart Guide\\n Contents\\n  \\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\nBy Harrison Chase\\n    \\n      ¬© Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Mar 24, 2023.',\n",
       "  'source': 'https://langchain.readthedocs.io\\\\en\\\\latest\\\\index.html'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{'id': f'{uid}-{i}', 'text': chunk, 'source': url} for i, chunk in enumerate(chunks)]\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the same logic across our full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matte\\miniconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 397/397 [00:04<00:00, 82.49it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2237"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "documents = []\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    url = doc.metadata['source'].replace('rtdocs\\\\', 'https://')\n",
    "    m.update(url.encode('utf-8'))\n",
    "    uid = m.hexdigest()[:12]\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            'id': f'{uid}-{i}',\n",
    "            'text': chunk,\n",
    "            'source': url\n",
    "        })\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now left with `2201` documents. We can save them to a JSON lines (`.jsonl`) file like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '94d948fe82d2-0',\n",
       " 'text': '.md\\n.pdf\\nDeployments\\n Contents \\nStreamlit\\nGradio (on Hugging Face)\\nBeam\\nVercel\\nSteamShip\\nDeployments#\\nSo you‚Äôve made a really cool chain - now what? How do you deploy it and make it easily sharable with the world?\\nThis section covers several options for that.\\nNote that these are meant as quick deployment options for prototypes and demos, and not for production systems.\\nIf you are looking for help with deployment of a production system, please contact us directly.\\nWhat follows is a list of template GitHub repositories aimed that are intended to be\\nvery easy to fork and modify to use your chain.\\nThis is far from an exhaustive list of options, and we are EXTREMELY open to contributions here.\\nStreamlit#\\nThis repo serves as a template for how to deploy a LangChain with Streamlit.\\nIt implements a chatbot interface.\\nIt also contains instructions for how to deploy this app on the Streamlit platform.\\nGradio (on Hugging Face)#\\nThis repo serves as a template for how deploy a LangChain with Gradio.\\nIt implements a chatbot interface, with a ‚ÄúBring-Your-Own-Token‚Äù approach (nice for not wracking up big bills).\\nIt also contains instructions for how to deploy this app on the Hugging Face platform.\\nThis is heavily influenced by James Weaver‚Äôs excellent examples.\\nBeam#\\nThis repo serves as a template for how deploy a LangChain with Beam.\\nIt implements a Question Answering app and contains instructions for deploying the app as a serverless REST API.\\nVercel#\\nA minimal example on how to run LangChain on Vercel using Flask.\\nSteamShip#\\nThis repository contains LangChain adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship.',\n",
       " 'source': 'https://langchain.readthedocs.io\\\\en\\\\latest\\\\deployments.html'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train.jsonl', 'w') as f:\n",
    "    for doc in documents:\n",
    "        f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data from file we'd write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2237"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "with open('train.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        documents.append(json.loads(line))\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '94d948fe82d2-0',\n",
       " 'text': '.md\\n.pdf\\nDeployments\\n Contents \\nStreamlit\\nGradio (on Hugging Face)\\nBeam\\nVercel\\nSteamShip\\nDeployments#\\nSo you‚Äôve made a really cool chain - now what? How do you deploy it and make it easily sharable with the world?\\nThis section covers several options for that.\\nNote that these are meant as quick deployment options for prototypes and demos, and not for production systems.\\nIf you are looking for help with deployment of a production system, please contact us directly.\\nWhat follows is a list of template GitHub repositories aimed that are intended to be\\nvery easy to fork and modify to use your chain.\\nThis is far from an exhaustive list of options, and we are EXTREMELY open to contributions here.\\nStreamlit#\\nThis repo serves as a template for how to deploy a LangChain with Streamlit.\\nIt implements a chatbot interface.\\nIt also contains instructions for how to deploy this app on the Streamlit platform.\\nGradio (on Hugging Face)#\\nThis repo serves as a template for how deploy a LangChain with Gradio.\\nIt implements a chatbot interface, with a ‚ÄúBring-Your-Own-Token‚Äù approach (nice for not wracking up big bills).\\nIt also contains instructions for how to deploy this app on the Hugging Face platform.\\nThis is heavily influenced by James Weaver‚Äôs excellent examples.\\nBeam#\\nThis repo serves as a template for how deploy a LangChain with Beam.\\nIt implements a Question Answering app and contains instructions for deploying the app as a serverless REST API.\\nVercel#\\nA minimal example on how to run LangChain on Vercel using Flask.\\nSteamShip#\\nThis repository contains LangChain adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship.',\n",
       " 'source': 'https://langchain.readthedocs.io\\\\en\\\\latest\\\\deployments.html'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Sharing the Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now created our dataset and you can go ahead and use it in any way you like. However, if you'd like to share the dataset, or store it somewhere that you can get easy access to later ‚Äî we can use [Hugging Face Datasets Hub](https://huggingface.co/datasets).\n",
    "\n",
    "To begin we first need to create an account by clicking the **Sign Up** button at [huggingface.co](https://huggingface.co/). Once done we click our profile button in the same location > click **New Dataset** > give it a name like *\"langchain-docs\"* > set the dataset to **Public** or **Private** > click **Create dataset**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
